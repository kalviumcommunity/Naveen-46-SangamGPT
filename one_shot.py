import os
import google.generativeai as genai
from dotenv import load_dotenv

load_dotenv()

try:
    api_key = os.environ.get("GOOGLE_API_KEY")
    if not api_key:
        raise ValueError("GOOGLE_API_KEY not found in .env file or environment.")
    genai.configure(api_key=api_key)
except (ValueError, TypeError) as e:
    print(f"Error: {e}")
    exit()


def log_token_usage(response, prompt_text: str, query_type: str = "One-Shot Query"):
    """
    Log token usage information for the API call.
    """
    try:
        usage_metadata = response.usage_metadata
        
        print(f"\nü™ô TOKEN USAGE - {query_type}")
        print("-" * 40)
        print(f"üìù Prompt Characters: {len(prompt_text)}")
        print(f"üìÑ Response Characters: {len(response.text)}")
        
        if hasattr(usage_metadata, 'prompt_token_count'):
            print(f"üî§ Input Tokens: {usage_metadata.prompt_token_count}")
        if hasattr(usage_metadata, 'candidates_token_count'):
            print(f"üî§ Output Tokens: {usage_metadata.candidates_token_count}")
        if hasattr(usage_metadata, 'total_token_count'):
            print(f"üî§ Total Tokens: {usage_metadata.total_token_count}")
            # Rough cost estimate
            estimated_cost = usage_metadata.total_token_count * 0.00015 / 1000
            print(f"üí∞ Estimated Cost: ~${estimated_cost:.6f}")
        
        print("-" * 40)
        
    except Exception as e:
        print(f"‚ö†Ô∏è Token usage not available: {e}")
        print(f"üìè Prompt: {len(prompt_text)} chars, Response: {len(response.text)} chars")
        print("-" * 40)


def get_historical_period(ruler_name):
    """
    Uses one-shot prompting to identify the historical period of a ruler.
    """
    
    prompt = (
        "You are a historian. Identify the historical period/era of the given ruler.\n\n"
        "Ruler: Julius Caesar\n"
        "Period: Ancient Rome (1st century BCE)\n\n"
        f"Ruler: {ruler_name}\n"
        "Period:"
    )

    print(f"--- Sending One-Shot Prompt to Gemini (Top P: 0.7) ---\n{prompt}\n---------------------------\n")

    try:
        model = genai.GenerativeModel('gemini-1.5-flash')
        response = model.generate_content(
            prompt,
            generation_config={
                "temperature": 0.5,
                "top_p": 0.7,  # Balanced vocabulary for historical facts
                "top_k": 40,
                "max_output_tokens": 150,
            }
        )
        
        # Log token usage
        log_token_usage(response, prompt, "One-Shot Historical Query")
        
        result = response.text.strip()
        return result

    except Exception as e:
        print(f"An error occurred: {e}")
        return None

# Example usage
ruler = "akbar"
period = get_historical_period(ruler)

if period:
    print(f"Historical Period for {ruler}:\n{period}")
